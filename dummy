--apps.PO_LINE_TYPES ---in oracle
UNIT_PRICE	NUMBER
UNIT_OF_MEASURE	VARCHAR2 (25 Byte)
ROW_ID	ROWID
REQUEST_ID	NUMBER
RECEIVING_FLAG	VARCHAR2 (1 Byte)
RECEIVE_CLOSE_TOLERANCE	NUMBER
PURCHASE_BASIS	VARCHAR2 (30 Byte)
PROGRAM_UPDATE_DATE	DATE
PROGRAM_ID	NUMBER
PROGRAM_APPLICATION_ID	NUMBER
OUTSIDE_OPERATION_FLAG	VARCHAR2 (1 Byte)
ORDER_TYPE_LOOKUP_CODE	VARCHAR2 (25 Byte)
MATCHING_BASIS	VARCHAR2 (30 Byte)
LINE_TYPE_ID	NUMBER
LINE_TYPE	VARCHAR2 (25 Byte)
LAST_UPDATE_LOGIN	NUMBER
LAST_UPDATE_DATE	DATE
LAST_UPDATED_BY	NUMBER
INACTIVE_DATE	DATE
DESCRIPTION	VARCHAR2 (240 Byte)
CREATION_DATE	DATE
CREATED_BY	NUMBER
CLM_SEVERABLE_FLAG	VARCHAR2 (1 Byte)
CATEGORY_ID	NUMBER
ATTRIBUTE_CATEGORY	VARCHAR2 (30 Byte)
ATTRIBUTE9	VARCHAR2 (150 Byte)
ATTRIBUTE8	VARCHAR2 (150 Byte)
ATTRIBUTE7	VARCHAR2 (150 Byte)
ATTRIBUTE6	VARCHAR2 (150 Byte)
ATTRIBUTE5	VARCHAR2 (150 Byte)
ATTRIBUTE4	VARCHAR2 (150 Byte)
ATTRIBUTE3	VARCHAR2 (150 Byte)
ATTRIBUTE2	VARCHAR2 (150 Byte)
ATTRIBUTE15	VARCHAR2 (150 Byte)
ATTRIBUTE14	VARCHAR2 (150 Byte)
ATTRIBUTE13	VARCHAR2 (150 Byte)
ATTRIBUTE12	VARCHAR2 (150 Byte)
ATTRIBUTE11	VARCHAR2 (150 Byte)
ATTRIBUTE10	VARCHAR2 (150 Byte)
ATTRIBUTE1	VARCHAR2 (150 Byte)

--apps.PO_LINE_TYPES  in hive
  `attribute3` string, 
  `attribute4` string, 
  `attribute5` string, 
  `attribute6` string, 
  `attribute7` string, 
  `attribute8` string, 
  `attribute9` string, 
  `attribute10` string, 
  `attribute11` string, 
  `attribute12` string, 
  `attribute13` string, 
  `attribute14` string, 
  `attribute15` string, 
  `outside_operation_flag` string, 
  `request_id` double, 
  `program_application_id` double, 
  `program_id` double, 
  `program_update_date` string, 
  `receive_close_tolerance` double, 
  `line_type_id` double, 
  `last_update_date` string, 
  `last_updated_by` double, 
  `order_type_lookup_code` string, 
  `last_update_login` double, 
  `creation_date` string, 
  `created_by` double, 
  `category_id` double, 
  `unit_of_measure` string, 
  `unit_price` double, 
  `receiving_flag` string, 
  `inactive_date` string, 
  `attribute_category` string, 
  `attribute1` string, 
  `attribute2` string, 
  `line_type` string, 
  `description` string, 
  `purchase_basis` string, 
  `matching_basis` string, 
  `clm_severable_flag` string)

  --apps.HR_ORGANIZATION_UNITS -in oracle
  
  TYPE	VARCHAR2 (30 Byte)
SOFT_CODING_KEYFLEX_ID	NUMBER (15)
ROW_ID	ROWID
REQUEST_ID	NUMBER (15)
PROGRAM_UPDATE_DATE	DATE
PROGRAM_ID	NUMBER (15)
PROGRAM_APPLICATION_ID	NUMBER (15)
ORGANIZATION_ID	NUMBER (15)
OBJECT_VERSION_NUMBER	NUMBER (9)
NAME	VARCHAR2 (240 Byte)
LOCATION_ID	NUMBER (15)
LAST_UPDATE_LOGIN	NUMBER (15)
LAST_UPDATE_DATE	DATE
LAST_UPDATED_BY	NUMBER (15)
INTERNAL_EXTERNAL_FLAG	VARCHAR2 (30 Byte)
INTERNAL_ADDRESS_LINE	VARCHAR2 (80 Byte)
DATE_TO	DATE
DATE_FROM	DATE
CREATION_DATE	DATE
CREATED_BY	NUMBER (15)
COST_ALLOCATION_KEYFLEX_ID	NUMBER (9)
COMMENTS	CLOB
BUSINESS_GROUP_ID	NUMBER (15)
ATTRIBUTE_CATEGORY	VARCHAR2 (30 Byte)
ATTRIBUTE9	VARCHAR2 (150 Byte)
ATTRIBUTE8	VARCHAR2 (150 Byte)
ATTRIBUTE7	VARCHAR2 (150 Byte)
ATTRIBUTE6	VARCHAR2 (150 Byte)
ATTRIBUTE5	VARCHAR2 (150 Byte)
ATTRIBUTE4	VARCHAR2 (150 Byte)
ATTRIBUTE30	VARCHAR2 (150 Byte)
ATTRIBUTE3	VARCHAR2 (150 Byte)
ATTRIBUTE29	VARCHAR2 (150 Byte)
ATTRIBUTE28	VARCHAR2 (150 Byte)
ATTRIBUTE27	VARCHAR2 (150 Byte)
ATTRIBUTE26	VARCHAR2 (150 Byte)
ATTRIBUTE25	VARCHAR2 (150 Byte)
ATTRIBUTE24	VARCHAR2 (150 Byte)
ATTRIBUTE23	VARCHAR2 (150 Byte)
ATTRIBUTE22	VARCHAR2 (150 Byte)
ATTRIBUTE21	VARCHAR2 (150 Byte)
ATTRIBUTE20	VARCHAR2 (150 Byte)
ATTRIBUTE2	VARCHAR2 (150 Byte)
ATTRIBUTE19	VARCHAR2 (150 Byte)
ATTRIBUTE18	VARCHAR2 (150 Byte)
ATTRIBUTE17	VARCHAR2 (150 Byte)
ATTRIBUTE16	VARCHAR2 (150 Byte)
ATTRIBUTE15	VARCHAR2 (150 Byte)
ATTRIBUTE14	VARCHAR2 (150 Byte)
ATTRIBUTE13	VARCHAR2 (150 Byte)
ATTRIBUTE12	VARCHAR2 (150 Byte)
ATTRIBUTE11	VARCHAR2 (150 Byte)
ATTRIBUTE10	VARCHAR2 (150 Byte)
ATTRIBUTE1	VARCHAR2 (150 Byte)
---- HR_ORGANIZATION_UNITS in oracle
  `organization_id` double, 
  `row_id` string, 
  `business_group_id` double, 
  `cost_allocation_keyflex_id` double, 
  `location_id` double, 
  `soft_coding_keyflex_id` double, 
  `date_from` string, 
  `name` string, 
  `comments` string, 
  `date_to` string, 
  `internal_external_flag` string, 
  `internal_address_line` string, 
  `type` string, 
  `request_id` double, 
  `program_application_id` double, 
  `program_id` double, 
  `program_update_date` string, 
  `attribute_category` string, 
  `attribute1` string, 
  `attribute2` string, 
  `attribute3` string, 
  `attribute4` string, 
  `attribute5` string, 
  `attribute6` string, 
  `attribute7` string, 
  `attribute8` string, 
  `attribute9` string, 
  `attribute10` string, 
  `attribute11` string, 
  `attribute12` string, 
  `attribute13` string, 
  `attribute14` string, 
  `attribute15` string, 
  `attribute16` string, 
  `attribute17` string, 
  `attribute18` string, 
  `attribute19` string, 
  `attribute20` string, 
  `last_update_date` string, 
  `last_updated_by` double, 
  `last_update_login` double, 
  `created_by` double, 
  `creation_date` string, 
  `object_version_number` double, 
  `attribute21` string, 
  `attribute22` string, 
  `attribute23` string, 
  `attribute24` string, 
  `attribute25` string, 
  `attribute26` string, 
  `attribute27` string, 
  `attribute28` string, 
  `attribute29` string, 
  `attribute30` string)




======

hive

Hive/Pig is well suited for batch processing (ETL, Scheduled workloads)

cat /etc/hive/conf // config files

cat hive-log4j.properties



hive(default)> set;  //displays all the properties

hive warehouse directory  : base directory in HDFS
====================================================
set hive.metastore.warehouse.dir;

set hive.execution.engine;


set dfs.replication;   //to check the replication factor

set dfs.replication=1 // changes


.hiverc  // used to control behaviour of hive session

set dfs.replication=2;
set hive.execution.engine=tez;




==================

hive --hiveconf hive.execution.engine=mr   // while launching the hive also, we can specify the properties


exit;  // to come out of hive




to run spark in hive

spark-sql --master --yarn



June 12, 2018
==================

Hive DDL and DML



set hive.metasore.warehouse.dir;


upto 11g oracle is single tenant

mysql is multi tenant 


diff b/w hive and mysql
===========================
in hive, we dont have users
in sql, we have users



June 14, 2018
==============

ctask

create
table
as
select

create table tablename as select * from another table;

ctrlA is the null value(ASCII)



serde is a file format to open a particular data


3 types to load data into hive tables
=====================================
insert individual columns
loading data from 1 table to another
using load



loading file from lFS to Hive using dfs -put (metadata info is not updated)

Load (metadata is updated)


using load 
directly put that files into hdfs

All the metadata info is present in NameNode

Datanode contains files, blocks associated with files(block location) is called metadata



try to load pdf data into table using LOAD


BEELINE/HUE EDITOR


truncate only deletes the data
ex: truncate table <tablename>


usecase
===========
input data is in text form
but we are supposed to store it in ORC format

Now we have to transform the text to ORC


Solution:

Create a stage table (which can accomdate the incoming data)
Load data into stage table
Apply transformations
(SELECT * FROM orders_orc select * from orders_stage;)
	To apply transformation, we use SELECT

then copy the data into table

Transformations can be done using hive query, MR, Spark, Pig

first load data into order_stage as text format
next load into orders_orc as select * from orders_Stage by mentioning orc format
===========

INSERT INTO <tablename>
INSERT OVERWRITE 

into is for append
overwrite is for overwriting


LOAD is used whenever the data is consistent
if it is not consistent, it will be a 2 step process
create stage table and load the data
apply transformation and send the data to the table



=================================================

partitioning



Dynamic partition



June 18, 2018
===============

Dynamic Partitioning

set hive.exec.dynamic.partition.mode=nonstrict;

set hive.metastore.warehouse.dir;  	# Shows the datawarehouse directory`	

Differences b/w managed and external tables

managed table - managed by hive

droping external table- all schema is deleted but not data. If we use select * from table it throws table not found

managed table with location
managed table without location ----- commonly used scenario
external table with location  ----- commonly used scenario
external table without location 


Business case
==============

create external table for staging the data
query external tables and insert into final tables



Bucketing
==========

Partitioning in RDBMS
---------------------
	List Partitioning
	Hash
	Range

Partitioning in Hive
---------------------
	List
	Hash (it can be achieved by hash i.e., using clustering)

Hashcode : If it integer, typically it will return the same value as hashcode
		   If it is non-integer, it returns 


Bucketing
=========
No of buckets - ex 4
Compute the hash code
Apply the mod algorithm with 4
whatever the remainder is put into the bucket (that bucket is called as hash bucket)



CREATE TABLE orders_bucketed (
order_id INT,
order_date STRING,
order_customer_id INT,
order_status STRING)
CLUSTERED BY (order_status) INTO 4 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';


INSERT INTO TABLE orders_bucketed
SELECT * 
FROM orders01;


Here numFiles=1 
Do you think bucketing is done ?
No

we have to set a property file
SET hive.enforce.bucketing=true;

TRUNCATE TABLE orders_bucketed;

INSERT INTO TABLE orders_bucketed
SELECT * 
FROM orders01;



================================================================================





SET hive.enforce.bucketing=true;

If you forgot the property we can find it using the following command

hive -e "set;"|grep bucket

hive -e "set;"|grep partition  // find properties related to partitioning
