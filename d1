import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._

def MySquerResult()
{

val partitionWindow = """OVER( PARTITION BY ord_id,ord_juln_d,fbsi_brch_c,fbsi_base_c ORDER BY cap_tmst,ord_audt_id  
ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)"""
  

val Frist_Dataframe = spark.sql(s"""
select ord_id,
ord_juln_d,
fbsi_brch_c,
fbsi_base_c,
ent_rp_usr_id,
cusp_n,
LAST_VALUE(cusp_n) $partitionWindow AS last_cusp_n,
FIRST_VALUE(ent_rp_usr_id) $partitionWindow first_ent_rp_usr_id,
FIRST_VALUE(signon_rp_c) $partitionWindow first_signon_rp_c,
LAST_VALUE(symb_c) $partitionWindow last_symb_c,
LAST_VALUE(lmtp_a) $partitionWindow last_lmtp_a,
LAST_VALUE(tt_exec_q) $partitionWindow last_tt_exec_q,
LAST_VALUE(avg_exec_prc_a) $partitionWindow last_avg_exec_prc_a,
ROW_NUMBER() OVER(PARTITION BY ord_id,ord_juln_d,fbsi_brch_c,fbsi_base_c ORDER BY cap_tmst,ord_audt_id) rn,
replaced_ord_id
from JobSupport.stg_oroa_audt_orders""");

val columns1and2_count = Window.partitionBy("ord_id", "ord_juln_d","fbsi_brch_c","fbsi_base_c");

val columns1and2_rank = Window.partitionBy("ord_id", "ord_juln_d","fbsi_brch_c","fbsi_base_c").orderBy("ord_id");

val cnt =Frist_Dataframe.count
 
val Dataframe_add_rank = Frist_Dataframe.withColumn("count", lit(cnt))

val Dataframe_add_rank_count = Dataframe_add_rank.withColumn("rank", rank() over columns1and2_rank).distinct()

val Dataframe_add_rank_count_xi = Dataframe_add_rank_count.withColumn("replaced_ord_id_xi", count($"replaced_ord_id") over columns1and2_count).
withColumn("cusp_n_xi", count($"cusp_n") over columns1and2_count)
//Add p(i)=maxrank(x(i))/count
val Dataframe_add_rank_count_xi_pi = Dataframe_add_rank_count_xi.withColumn("replaced_ord_id_pi", $"replaced_ord_id_xi" / $"count").
withColumn("cusp_n_pi", $"cusp_n_xi" / $"count")
Dataframe_add_rank_count_xi_pi.show()
Dataframe_add_rank_count_xi_pi.createOrReplaceTempView("MyAnalyticalFunctionData") //temporary table creation in hive
}
